# -*- coding: utf-8 -*-
"""iris.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fofE96tpH5eMEO3PfPHBlgUAfE7wu9pv

**IRIS DATASET**

In this notebook we are going to implement a model for iris dataset in tensorflow

**What is IRIS Dataset ?**

The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher
in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. 
It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. 
Two of the three species were collected in the Gasp√© Peninsula "all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus"
"""

from sklearn.datasets import load_iris
from sklearn import preprocessing
import tensorflow as tf

data = load_iris()
print(data['target'])
y = preprocessing.OneHotEncoder(sparse=False).fit_transform(data['target'].reshape(-1, 1))
print(y)

x = preprocessing.MinMaxScaler(feature_range=(-1,+1)).fit_transform(data['data'])
print(y.shape)
y_true_cls = tf.argmax(y,axis=1)
print(y_true_cls)

input_x = tf.placeholder(tf.float32,shape = (150,4))
input_y = tf.placeholder(tf.float32,shape=(150,3))
weigths = tf.Variable(tf.zeros([tf.shape(input_x)[1],3]))
bias = tf.Variable(tf.zeros(([150,3])))
shape = tf.shape(input_x)[1]
m = tf.add(weights,weights)
mat = tf.add(tf.matmul(input_x,weights),bias)
k = tf.argmax(mat,axis=1)
loss = tf.nn.softmax_cross_entropy_with_logits(logits = mat,
                                                       labels = y)
cost = tf.reduce_mean(loss)
# loss = tf.metrics.root_mean_squared_error(y_true_cls, k)
# cost = tf.reduce_mean(loss)
optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.02).minimize(cost)
correct_prediction = tf.equal(k, data['target'])
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

init_op = tf.initialize_all_variables()
with tf.Session() as sess:
  sess.run(init_op)
  for i in range(1000):
    p = sess.run(optimizer,feed_dict= {input_x:x,input_y:y})
    if i%100 == 0:
      print("Training accuracy at {}th epoch {}".format(i,sess.run(accuracy,feed_dict= {input_x:x,input_y:y})))

