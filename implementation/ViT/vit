What is attention ?

transformers especially vit are better in large data
in vit image are split into patches and converted into embeddings 

self attention blocks replacing CNN ?
