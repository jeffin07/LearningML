

## Attention is all you need

This is a very minimal implementation of the paper [attention is all you need ](https://arxiv.org/pdf/1706.03762v5.pdf)  

The main idea in this paper is self-attention and the transformer architecture  

Which is foccused here
